{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n",
      "(\"civil war\" OR \"violence\" OR \"violent\" OR \"conflict\" OR \"fight*\" OR \"killing\" OR \"battle\" OR \"massacre\" OR \"injury\" OR \"butchery\" OR \"explosion\" OR \"car bombs\" OR \"car bombs\" OR \"corpse\" OR \"abduction\" OR \"ambush*\" OR \"suicide\" OR \"suicide bomber*\" OR \"rape\" OR \"persecution\" OR \"assassination\" OR \"terror*\" OR \"militia\" OR \"military base\" OR \"attack*\" OR \"assault\" OR \"gang\" OR \"crime\" OR \"theft\" OR \"clash*\" OR \"fighter*\" OR \"mortar\" OR \"rocket\" OR \"siege\" OR \"shelling\" OR \"shells\" OR \"force*\" OR \"bomb\" OR \"warring\" OR \"gun*\" OR \"soldiers\" OR \"rebel\" OR \"unrest\" OR \"troubled\" OR \"insurgent\" OR \"dead\" OR \"crisis\" OR \"victim\" OR \"detention\" OR \"detention\" OR \"detainee\" OR \"detainee\" OR \"prisoner\" OR \"prisoner\" OR \"detention centre\" OR \"detention centre\" OR \"indiscriminate\" OR \"checkpoint\" OR \"checkpoint\" OR \"operation\" OR \"detention centre\" OR \"torture\" OR \"elements\" OR \"execute\" OR \"execution\" OR \"withdrawal\" OR \"shelling\" OR \"feud\" OR \"weapons\" OR \"kidnapping\" OR \"body\" OR \"ransom\" OR \"ammunition\" OR \"martyr\" OR \"martyred\" OR \"roadside bomb\" OR \"injured\" OR \"rounds\" OR \"patrol\" OR \"victim\" OR \"target\" OR \"target (n)\" OR \"fighter*\" OR \"revolutionary\" OR \"killed\" OR \"blockade\" OR \"arrest\") AND (\"Al Sa'adiya\" OR \"Al Hay Al Wasity\" OR \"Al Wihda\" OR \"Sumel\" OR \"Al Khidhir\" OR \"Al Kuwair\" OR \"Al Noor\" OR \"Diwaniya\" OR \"Tikrit\" OR \"Mansour\" OR \"Al Warka'a\" OR \"Al Dhuluiya\" OR \"Rutba\" OR \"Al Rifa'i\" OR \"Al Hawiga, Al Hawija, Hawija\" OR \"Al Saray\" OR \"Afaq\" OR \"Ain Al Tamur\" OR \"Al Khalis\" OR \"Bab Al Mashhad\" OR \"Debiss\" OR \"Darbandokeh\" OR \"Abu Ghraib, Abu Ghreib\" OR \"Al Qibla\" OR \"Bijil\" OR \"syria*\" OR \"Al Herra\" OR \"Yusufiya\" OR \"Al Bayyaa\" OR \"Sa'ad, Saad\" OR \"Pshdar\" OR \"Al Madaen\" OR \"Al Yousifiya\" OR \"Al Sheekhan\" OR \"Al Hindiya\" OR \"Shatt al-Arab\" OR \"Al Dhubbat\" OR \"Karkh\" OR \"Al Ibilla\" OR \"Zayouna\" OR \"Hubhib\" OR \"Karmat Beni Sa'eed\" OR \"Ain Tamr\" OR \"Al Hartha\" OR \"Al Jibayesh\" OR \"Shaab, Shabb\" OR \"Al Hay Al Sena'ye\" OR \"Al Talee'a\" OR \"Akre\" OR \"Al Ba'aj\" OR \"Khan al-Baghdadi\" OR \"Tuz Khormato, Tuz Khurmatu\" OR \"Garma, Karma\" OR \"Baghdad\" OR \"Kut\" OR \"Al Majid\" OR \"Al Shumookh\" OR \"Hay Alhussain\" OR \"Al-Jadriya\" OR \"Qal'at Saleh\" OR \"Karabila\" OR \"Rusafa\" OR \"Haditha\" OR \"Mishkhab\" OR \"Hatra\" OR \"Baquba, Baqubah\" OR \"Al Hai\" OR \"Amadiya\" OR \"Koi Sanjaq\" OR \"Kalar\" OR \"Latifiya, Latifiyah\" OR \"Al Latifiya\" OR \"Safwan\" OR \"Wasit, Wassit\" OR \"Ana\" OR \"Al Sanniya\" OR \"Ibnil Haythem\" OR \"Halabja City\" OR \"Salah Ad Din, Salah al Din, Salahadin\" OR \"Al Meethaq\" OR \"Al Wajehiya\" OR \"Rania\" OR \"Al Shuraa\" OR \"Al Maymouna\" OR \"Mahmudiya\" OR \"Mandily\" OR \"Baghdad City\" OR \"Rawa\" OR \"Al Khan'saa\" OR \"Nukhayb\" OR \"Al Nile\" OR \"Hay Adan\" OR \"Hitteen\" OR \"Sharazoor\" OR \"Baiji\" OR \"Shoala/Shuala\" OR \"Al Rashdiya\" OR \"Al Salman\" OR \"Hamourabi\" OR \"Ghazaliya\" OR \"Husseiniyah, Husseiniya\" OR \"Najaf CIty\" OR \"Al Abbasiya\" OR \"Al Mashrooa'\" OR \"Sab' Albour\" OR \"Amil\" OR \"Al Bat'haa\" OR \"Zafraniya, Zafaraniya\" OR \"Rijal Al A'mal\" OR \"Al Salam\" OR \"Abu Sayda\" OR \"Batufa\" OR \"Al Dawoodi\" OR \"Tel Keppe, Tel Keef\" OR \"Al Rashid\" OR \"Al Obaidi\" OR \"Kufa\" OR \"Sulaymaniya, Sulaymaniyah, Sulaimaniya, Suleimaniya, Suleimaniyah, Suleymaniyya\" OR \"Qaratabba\" OR \"Mahawil, Mahaweel\" OR \"Hilla\" OR \"Babil\" OR \"Qurna\" OR \"Al Abassi\" OR \"Al Mejar Al Kabi\" OR \"Al Qadisiya\" OR \"Samarra, Samara\" OR \"Saqlawiya\" OR \"Kadhimyah, Kadhimiya\" OR \"Al Thawra\" OR \"Sadr City\" OR \"Basra, Al Basra, Basrah, Al Basrah\" OR \"Al Ez\" OR \"Al Rumaitha\" OR \"Al Dawaya\" OR \"Yarmouk\" OR \"Al Tarmiya, Tarmiya, Tarmiyah, Tarmia\" OR \"Mosul, Al Mosul\" OR \"Nassriya, Nasriyah, Nasiriyah, Nasriya\" OR \"Sinjar\" OR \"Al Dbeer\" OR \"Maqal\" OR \"Musayab, Musayyib, Musayib, Musayyab\" OR \"Al Dair\" OR \"Soran\" OR \"Al Ahrar\" OR \"Abi Tammam\" OR \"Um Qasr\" OR \"Al Hamdaniya/Baghdeda\" OR \"Rahaliya\" OR \"Bartellah\" OR \"Sulaymaniya City\" OR \"Al Masarif\" OR \"Al Intisar\" OR \"Al Salihiya\" OR \"Kifri\" OR \"Urr, Ur, Orr\" OR \"Al Muqdadiya, Muqdadiya\" OR \"Al Hadher\" OR \"Al Azeeziaya\" OR \"Amara\" OR \"Al Zuhoor\" OR \"Babylon\" OR \"Washash\" OR \"Al Maimouna\" OR \"Telkaif\" OR \"Al Kehlaa\" OR \"Al Risala\" OR \"Al Istiqlal\" OR \"Nahiyat Alrasheed\" OR \"Al Eshaqi\" OR \"Haqlaniya\" OR \"Amiriyat Fallujah\" OR \"Al Muntheriya\" OR \"Al Hammar\" OR \"Al Kahla\" OR \"Kumait\" OR \"Tammouz\" OR \"Heet, Hit\" OR \"Akashat\" OR \"Al Asmaee\" OR \"Babel\" OR \"Ghammas\" OR \"Khour Alzubair\" OR \"Al Fetheliya\" OR \"Al Showmali\" OR \"Al Ton Kubri\" OR \"Al Qassim\" OR \"Al Midhatiya\" OR \"Al Quma\" OR \"Al Qayara\" OR \"Bab al-Moatham\" OR \"Makhmour\" OR \"Ali Alsharqi\" OR \"Al Shafiiyya\" OR \"Al Manathera\" OR \"Diyala, Diala, Diyalah\" OR \"Palestine Street\" OR \"Mergasur\" OR \"Walid, Waleed\" OR \"Kan’aan, Kanaan\" OR \"Khanaqin\" OR \"Abi Gharaq\" OR \"Iskandariya\" OR \"Taji\" OR \"Al Mandiqa Al Hura\" OR \"Dokan\" OR \"Al Muwafaqiya\" OR \"Saidiya\" OR \"Al Gharraf\" OR \"Al Shatra\" OR \"Zakho\" OR \"Al Zab\" OR \"Eshtar\" OR \"Qal'at Salih\" OR \"Belatt Al Shuhada'a\" OR \"Zummar\" OR \"Rabeeaa\" OR \"Al Riyadh\" OR \"Al Tahaddi\" OR \"Hashimiya\" OR \"Qadisiyya, Qadisiya, Qadissiyah, Qadissiya\" OR \"Suq Al-Shoyokh\" OR \"iraq*\" OR \"Al Na'maniya\" OR \"Al Islah\" OR \"Karbala, Kerbala, Karbalah, Kerbalah\" OR \"Amiriya\" OR \"Bani Sa'ad\" OR \"Mahmoudiya\" OR \"New Baghdad\" OR \"Faw, Fao\" OR \"Ubaidi, Obaidi\" OR \"Dohuk\" OR \"Al Iskan\" OR \"Al Shirqat, Shirqat\" OR \"Hayfa\" OR \"Green Zone\" OR \"Al Ilwiya\" OR \"Tuz Khormato\" OR \"Talha\" OR \"Al Muthanna, Muthanna\" OR \"Al Fuhood\" OR \"Habbaniya\" OR \"Anbar Al Anbar\" OR \"Erbil, Irbil, Arbil\" OR \"Al Najmi\" OR \"Makhmur\" OR \"Sab’abkar\" OR \"Al Qul'aa\" OR \"Penjwin\" OR \"Hamdan\" OR \"Al Nasr\" OR \"Al Chibayish\" OR \"Hammam Al Aleel\" OR \"Ninewa, Ninewah, Ninevah, Nineveh, Ninawa, Ninawah\" OR \"Al Emam, Al Imam\" OR \"Al Mansouryah\" OR \"Baladrooz, Balad Ruz\" OR \"Qazaniyah\" OR \"Al Jumhouriya\" OR \"Al Salahiya\" OR \"Najaf, Al Najaf\" OR \"Jurf al-Sakhar\" OR \"Waziriya\" OR \"Chamchamal\" OR \"Al Misharrah\" OR \"Abu al-Khaseeb\" OR \"Al Qoush\" OR \"Barwana\" OR \"Tel Afar, Tal Afar, Talafar, Telafar\" OR \"Qaim\" OR \"Kifl\" OR \"Ali Al Gharbi\" OR \"Badra\" OR \"Nefar\" OR \"Al Nashwa\" OR \"Kabisa\" OR \"Al Mahannawiya\" OR \"Al Baladiyat\" OR \"Shaqlawa\" OR \"Treibil, Karama\" OR \"Dhi Qar, Dhiqar, Thi Qar\" OR \"Al Mutannabi\" OR \"Choman\" OR \"Ba’sheeqa\" OR \"Al Fida'a\" OR \"Balad\" OR \"Falluja, Fallujah\" OR \"Dohuk, Dohuq, Dahuk, Dihuk, Dohok\" OR \"Doura, Dura\" OR \"Al Suwaira\" OR \"Zubaidya\" OR \"Kirkuk\" OR \"Al Hay Al Askary\" OR \"Al Shannafiya\" OR \"Sumer\" OR \"Al Sider\" OR \"Shekhan, Ain Sifni\" OR \"Dujail\" OR \"Halabja, Halabjah\" OR \"Al Helall\" OR \"Al Fajr\" OR \"Hamza\" OR \"Jisr Diyala\" OR \"Al Namroud\" OR \"Al Daghara\" OR \"Shaikh Sa'ad\" OR \"Husayba\" OR \"Baghdad City districts:\" OR \"Al Nasr Wal Salam\" OR \"AL Azair\" OR \"Daquq\" OR \"Al Dor\" OR \"Al Samawa\" OR \"Saddat al Hindiyah\" OR \"Maysan, Missan\" OR \"Al Sumood\" OR \"Ramadi\" OR \"Erbil City\" OR \"Zubair, Zubayr\" OR \"Al Seniyah\" OR \"Khalidiya\" OR \"Hurriya\" OR \"Saidsadiq\" OR \"Lilan\" OR \"Karadah, Karada, Karrada, Karradah\" OR \"Adhamiyah\" OR \"Jalawla'a, Jalawla\" OR \"Qaem\" OR \"Al Jihad\" OR \"Al Shamiya\" OR \"Tooz\" OR \"Al Aqsa\" OR \"Al Urooba\")\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/iraq_geo.csv')\n",
    "\n",
    "geo_loc = []\n",
    "topic = \"(\\\"\" +  '\" OR \"'.join(df['title']) + \"\\\") AND \" \n",
    "\n",
    "\n",
    "\n",
    "df = df.drop('title', axis=1)\n",
    "# df = df.dropna()\n",
    "\n",
    "# print(df)\n",
    "\n",
    "for column in df:\n",
    "    geo_loc.append(df[column])\n",
    "\n",
    "\n",
    "        \n",
    "geo_loc = [item for sublist in geo_loc for item in sublist if item != 'Nan']    \n",
    "geo_loc = list(set(filter(None, geo_loc)) )\n",
    "geo_loc.pop(0)\n",
    "print(len(geo_loc))\n",
    "\n",
    "\n",
    "topic = topic +  \"(\\\"\" + '\" OR \"'.join(geo_loc) + \"\\\")\" \n",
    "\n",
    "print(topic) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOS preprocess library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import string\n",
    "import xmltodict\n",
    "import glob\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import _pickle as pickle\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"data/Karbala/en_2014-01-12_41da726433d291c1eefd349dab376dd96f7bdb3.xml\"\n",
    "# path = \"data/Karbala/*.xml\"\n",
    "path = \"/home/sonic/sonic/EOS_DATA/XML_Export_6-8-2015/ContainingTerms/English/*/*.xml\"\n",
    "review_txt_filepath = 'data/eos/review_text_all.txt'\n",
    "unigram_sentences_filepath = 'data/eos/unigram_sentences_all.txt'\n",
    "bigram_model_filepath = 'data/eos/bigram_model_all'\n",
    "bigram_sentences_filepath = 'data/eos/bigram_sentences_all.txt'\n",
    "trigram_model_filepath = 'data/eos/trigram_model_all'\n",
    "trigram_sentences_filepath = 'data/eos/trigram_sentences_all.txt'\n",
    "trigram_reviews_filepath = 'data/eos/trigram_transformed_reviews_all.txt'\n",
    "trigram_dictionary_filepath = 'data/eos/trigram_dict_all.dict'\n",
    "trigram_bow_filepath = 'data/eos/trigram_bow_corpus_all.mm'\n",
    "lda_model_filepath = 'data/eos/lda_model_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        for fname in glob.glob(path):\n",
    "            \n",
    "            with codecs.open(fname, encoding='utf_8') as doc_file:\n",
    "                # print(fname)\n",
    "                try:\n",
    "                    doc = xmltodict.parse(doc_file.read())\n",
    "                    # print(doc['Document']['Id'])\n",
    "                    # print(doc['Document']['Text'])\n",
    "                    # tokens = word_tokenize(doc['Document']['Text'].lower())\n",
    "                    review_txt_file.write(doc['Document']['Text'].replace('\\n', ' ').lower())\n",
    "                    review_count += 1\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "\n",
    "                    \n",
    "        print (u'''Text from {:,} documents written to the new txt file.'''.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print (u'Text from {:,} documents in the txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_doc = list(it.islice(f, 0, 1))[0]\n",
    "    sample_doc = sample_review.replace('\\\\n', '\\n')\n",
    "    \n",
    "parsed_review = nlp(sample_doc)\n",
    "print (parsed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print ('Sentence {}:'.format(num + 1))\n",
    "    print (sentence)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print ('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# token_text = [token.orth_ for token in parsed_review]\n",
    "# token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "# pd.DataFrame(list(zip(token_text, token_pos)), columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            # yield review.replace('\\\\n', '\\n')\n",
    "            yield review\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=100, n_threads=7):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 30min 21s, sys: 32 s, total: 3h 30min 53s\n",
      "Wall time: 1h 21min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.64 s, sys: 588 ms, total: 9.22 s\n",
      "Wall time: 9.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Bigrams\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 52s, sys: 2.94 s, total: 11min 55s\n",
      "Wall time: 11min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.81 s, sys: 392 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Main Funtion\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=100, n_threads=7):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "                        \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.en.language_data.STOP_WORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LineSentence' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LineSentence' object does not support indexing"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# tCreat trigram dictionary\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_reviews_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
