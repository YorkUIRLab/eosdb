{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# author - Richard Liao\n",
    "# Dec 26 2016\n",
    "# https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'violence/terrorism': 1, 'political': 3, 'misc': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    syrian assad say syria killed damascus people ...\n",
       "1    use osc copyrighted_material dissemination usa...\n",
       "2    will year can people one country party make sa...\n",
       "3    quot apos say the we it reuters terrorists ass...\n",
       "4    baghdad iraq sunni killed bomb iraqi attacks w...\n",
       "Name: topicFlat, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFB5JREFUeJzt3X/wXXV95/HniwQE1CVEYsqSYHCNOtgtmk0ZrHZXZdgC\nVkN3LMWxNcNkmp0tu6uys1t0Otp2d2famd2idFpqttgG118URbIuukbEtrsuYKLIT5EUoSSCiYAg\nYmWj7/3jfr71Jp7ke74k594vyfMxc+d+zud87j3v78nJ9/U959x7TqoKSZL2dsS0C5AkzU8GhCSp\nkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqNGhAJFmU5OokX0tyV5JXJlmcZHOSe9rz8W1sklyWZFuS\nW5OsGrI2SdL+Db0H8T7gM1X1UuA04C7gEuD6qloJXN+mAc4BVrbHeuDygWuTJO1HhvomdZLjgFuA\nF9bYQpLcDbymqh5MciLwhap6SZL3t/ZH9h63r2WccMIJtWLFikHql6RD1datW79dVUtmG7dwwBpO\nAXYBf5bkNGAr8DZg6dgv/YeApa19EvDA2Ou3t759BsSKFSvYsmXLwa5bkg5pSe7vM27IQ0wLgVXA\n5VX1CuB7/PhwEgBtz2JOuzBJ1ifZkmTLrl27DlqxkqQ9DRkQ24HtVXVTm76aUWB8qx1aoj3vbPN3\nAMvHXr+s9e2hqjZU1eqqWr1kyax7SJKkp2mwgKiqh4AHkrykdZ0J3AlsAta2vrXAta29CXhr+zTT\nGcBj+zv/IEka1pDnIAD+DfChJEcB9wIXMgqlq5KsA+4Hzm9jrwPOBbYBT7axkqQpGTQgquoWYHXH\nrDM7xhZw0ZD1SJL685vUkqROBoQkqZMBIUnqZEBIkjoN/SmmeevSzV+f2rLfcdaLp7ZsSerLPQhJ\nUicDQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJ\nUicDQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0WTrsASXqmunTz16e27Hec9eLBl+Ee\nhCSp06ABkeS+JLcluSXJlta3OMnmJPe05+Nbf5JclmRbkluTrBqyNknS/k1iD+K1VfXyqlrdpi8B\nrq+qlcD1bRrgHGBle6wHLp9AbZKkfZjGIaY1wMbW3gicN9Z/ZY3cCCxKcuIU6pMkMXxAFPDZJFuT\nrG99S6vqwdZ+CFja2icBD4y9dnvrkyRNwdCfYnp1Ve1I8nxgc5Kvjc+sqkpSc3nDFjTrAU4++eSD\nV6kkaQ+D7kFU1Y72vBO4Bjgd+NbMoaP2vLMN3wEsH3v5sta393tuqKrVVbV6yZIlQ5YvSYe1wQIi\nybOTPHemDfxz4HZgE7C2DVsLXNvam4C3tk8znQE8NnYoSpI0YUMeYloKXJNkZjkfrqrPJPkScFWS\ndcD9wPlt/HXAucA24EngwgFrkyTNYrCAqKp7gdM6+h8GzuzoL+CioeqRJM2N36SWJHUyICRJnQwI\nSVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwI\nSVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwI\nSVKnwQMiyYIkX0nyqTZ9SpKbkmxL8rEkR7X+Z7XpbW3+iqFrkyTt2yT2IN4G3DU2/fvApVX1IuBR\nYF3rXwc82vovbeMkSVMyaEAkWQa8HvjTNh3gdcDVbchG4LzWXtOmafPPbOMlSVMw9B7Ee4H/APyo\nTT8P+E5V7W7T24GTWvsk4AGANv+xNl6SNAWDBUSSXwR2VtXWg/y+65NsSbJl165dB/OtJUljhtyD\neBXwxiT3AR9ldGjpfcCiJAvbmGXAjtbeASwHaPOPAx7e+02rakNVra6q1UuWLBmwfEk6vA0WEFX1\nzqpaVlUrgAuAz1fVW4AbgDe1YWuBa1t7U5umzf98VdVQ9UmS9m8a34P4TeDiJNsYnWO4ovVfATyv\n9V8MXDKF2iRJzcLZhxy4qvoC8IXWvhc4vWPM3wG/PIl6JEmz85vUkqROBoQkqZMBIUnqZEBIkjoZ\nEJKkTgaEJKlTr4BI8o+HLkSSNL/03YP44yQ3J/mNJMcNWpEkaV7oFRBV9fPAWxhdK2lrkg8nOWvQ\nyiRJU9X7HERV3QP8FqNLZfwz4LIkX0vyL4YqTpI0Pb0utZHkZ4ALGd38ZzPwhqr6cpJ/CPxf4BPD\nlSg981y6+etTW/Y7znrx1JatQ0vfazH9IaO7wr2rqr4/01lV30zyW4NUJkmaqr4B8Xrg+1X1Q4Ak\nRwBHV9WTVfXBwaqTJE1N33MQnwOOGZs+tvVJkg5RfQPi6Kp6YmaitY8dpiRJ0nzQNyC+l2TVzESS\nfwJ8fz/jJUnPcH3PQbwd+Isk3wQC/BTwK4NVJUmaul4BUVVfSvJS4CWt6+6q+n/DlSVJmra53HL0\nZ4EV7TWrklBVVw5SlSRp6vp+Ue6DwD8CbgF+2LoLMCAk6RDVdw9iNXBqVdWQxUiS5o++n2K6ndGJ\naUnSYaLvHsQJwJ1JbgZ+MNNZVW8cpCpJ0tT1DYjfHrIISdL80/djrn+Z5AXAyqr6XJJjgQXDliZJ\nmqa+txz9deBq4P2t6yTgk0MVJUmavr4nqS8CXgU8Dn9/86DnD1WUJGn6+gbED6rqqZmJJAsZfQ9i\nn5Ic3e5j/dUkdyT5ndZ/SpKbkmxL8rEkR7X+Z7XpbW3+iqf3I0mSDoa+AfGXSd4FHNPuRf0XwP+Y\n5TU/AF5XVacBLwfOTnIG8PvApVX1IuBRYF0bvw54tPVf2sZJkqakb0BcAuwCbgP+JXAdo/tT71ON\nzFwi/Mj2KOB1jM5nAGwEzmvtNW2aNv/MJOlZnyTpIOv7KaYfAf+tPXpLsgDYCrwI+CPgb4DvVNXu\nNmQ7oxPetOcH2vJ2J3kMeB7w7bksU5J0cPS9FtM36DjnUFUv3N/r2i1KX55kEXAN8NKnU+RetawH\n1gOcfPLJB/p2kqR9mMu1mGYcDfwysLjvQqrqO0luAF4JLEqysO1FLAN2tGE7gOXA9nYS/Djg4Y73\n2gBsAFi9erXXhpKkgfQ6B1FVD489dlTVe4HX7+81SZa0PQeSHAOcBdwF3AC8qQ1bC1zb2pvaNG3+\n5704oCRNT99DTKvGJo9gtEcx22tPBDa28xBHAFdV1aeS3Al8NMl/Ar4CXNHGXwF8MMk24BHggv4/\nhiTpYOt7iOm/jrV3A/cB5+/vBVV1K/CKjv57gdM7+v+O0aErSdI80PdTTK8duhBJ0vzS9xDTxfub\nX1V/cHDKkSTNF3P5FNPPMjqRDPAG4GbgniGKkiRNX9+AWAasqqrvAiT5beB/VtWvDlWYJGm6+l5q\nYynw1Nj0U61PknSI6rsHcSVwc5Jr2vR5/Pi6SZKkQ1DfTzH95ySfBn6+dV1YVV8ZrixJ0rT1PcQE\ncCzweFW9j9HlME4ZqCZJ0jzQ95aj7wF+E3hn6zoS+O9DFSVJmr6+exC/BLwR+B5AVX0TeO5QRUmS\npq9vQDzVLpxXAEmePVxJkqT5oG9AXJXk/Ywu1f3rwOeY482DJEnPLH0/xfRf2r2oHwdeAry7qjYP\nWpkkaapmDYh2ue7PtQv2GQqSdJiY9RBTu23oj5IcN4F6JEnzRN9vUj8B3JZkM+2TTABV9W8HqUqS\nNHV9A+IT7SFJOkzsNyCSnFxVf1tVXndJkg4zs52D+ORMI8nHB65FkjSPzBYQGWu/cMhCJEnzy2wB\nUftoS5IOcbOdpD4tyeOM9iSOaW3adFXVPxi0OknS1Ow3IKpqwaQKkSTNL3O5H4Qk6TBiQEiSOhkQ\nkqROBoQkqdNgAZFkeZIbktyZ5I4kb2v9i5NsTnJPez6+9SfJZUm2Jbk1yaqhapMkzW7IPYjdwL+r\nqlOBM4CLkpwKXAJcX1UrgevbNMA5wMr2WA9cPmBtkqRZDBYQVfVgVX25tb8L3AWcBKwBZq7ttBE4\nr7XXAFfWyI2M7l534lD1SZL2byLnIJKsAF4B3AQsraoH26yHgKWtfRLwwNjLtrc+SdIUDB4QSZ4D\nfBx4e1U9Pj6vqoo5XsIjyfokW5Js2bVr10GsVJI0btCASHIko3D4UFXN3E/iWzOHjtrzzta/A1g+\n9vJlrW8PVbWhqlZX1eolS5YMV7wkHeaG/BRTgCuAu6rqD8ZmbQLWtvZa4Nqx/re2TzOdATw2dihK\nkjRhfe8o93S8Cvg1RrcqvaX1vQv4PeCqJOuA+4Hz27zrgHOBbcCTwIUD1iZJmsVgAVFV/5s97ycx\n7syO8QVcNFQ9kqS58ZvUkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4G\nhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4G\nhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqNFhAJPlAkp1Jbh/rW5xkc5J72vPxrT9JLkuyLcmt\nSVYNVZckqZ8h9yD+HDh7r75LgOuraiVwfZsGOAdY2R7rgcsHrEuS1MNgAVFVfwU8slf3GmBja28E\nzhvrv7JGbgQWJTlxqNokSbOb9DmIpVX1YGs/BCxt7ZOAB8bGbW99kqQpmdpJ6qoqoOb6uiTrk2xJ\nsmXXrl0DVCZJgskHxLdmDh21552tfwewfGzcstb3E6pqQ1WtrqrVS5YsGbRYSTqcTTogNgFrW3st\ncO1Y/1vbp5nOAB4bOxQlSZqChUO9cZKPAK8BTkiyHXgP8HvAVUnWAfcD57fh1wHnAtuAJ4ELh6pL\nktTPYAFRVW/ex6wzO8YWcNFQtUiS5s5vUkuSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmT\nASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmT\nASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqdO8CogkZye5O8m2JJdMux5JOpzN\nm4BIsgD4I+Ac4FTgzUlOnW5VknT4mjcBAZwObKuqe6vqKeCjwJop1yRJh635FBAnAQ+MTW9vfZKk\nKVg47QLmKsl6YH2bfCLJ3U/zrU4Avn1wqpqbi/c/e2p1zcK65ma+bl/gOpureVnXxQdW1wv6DJpP\nAbEDWD42vaz17aGqNgAbDnRhSbZU1eoDfZ+Dzbrmxrrmbr7WZl1zM4m65tMhpi8BK5OckuQo4AJg\n05RrkqTD1rzZg6iq3Un+NfC/gAXAB6rqjimXJUmHrXkTEABVdR1w3YQWd8CHqQZiXXNjXXM3X2uz\nrrkZvK5U1dDLkCQ9A82ncxCSpHnkkAuIJB9IsjPJ7fuYnySXtct53Jpk1di8tUnuaY+1E67rLa2e\n25J8MclpY/Pua/23JNky4bpek+Sxtuxbkrx7bN5gl0bpUde/H6vp9iQ/TLK4zRtyfS1PckOSO5Pc\nkeRtHWMmvo31rGvi21jPuia+jfWsa+LbWJKjk9yc5Kutrt/pGPOsJB9r6+SmJCvG5r2z9d+d5BcO\nuKCqOqQewD8FVgG372P+ucCngQBnADe1/sXAve35+NY+foJ1/dzM8hhdbuSmsXn3ASdMaX29BvhU\nR/8C4G+AFwJHAV8FTp1UXXuNfQPw+QmtrxOBVa39XODre//c09jGetY18W2sZ10T38b61DWNbaxt\nM89p7SOBm4Az9hrzG8CftPYFwMda+9S2jp4FnNLW3YIDqeeQ24Ooqr8CHtnPkDXAlTVyI7AoyYnA\nLwCbq+qRqnoU2AycPam6quqLbbkANzL6HsjgeqyvfRn00ihzrOvNwEcO1rL3p6oerKovt/Z3gbv4\nyW/8T3wb61PXNLaxnutrXwbbxp5GXRPZxto280SbPLI99j5RvAbY2NpXA2cmSev/aFX9oKq+AWxj\ntA6ftkMuIHrY1yU95tOlPtYx+gt0RgGfTbI1o2+ST9or2y7vp5O8rPXNi/WV5FhGv2Q/PtY9kfXV\ndu1fweivvHFT3cb2U9e4iW9js9Q1tW1stvU16W0syYIktwA7Gf1Bsc/tq6p2A48Bz2OA9TWvPuYq\nSPJaRv95Xz3W/eqq2pHk+cDmJF9rf2FPwpeBF1TVE0nOBT4JrJzQsvt4A/B/qmp8b2Pw9ZXkOYx+\nYby9qh4/mO99IPrUNY1tbJa6praN9fx3nOg2VlU/BF6eZBFwTZKfrqrOc3FDOxz3IPZ1SY9el/oY\nUpKfAf4UWFNVD8/0V9WO9rwTuIYD3G2ci6p6fGaXt0bfUzkyyQnMg/XVXMBeu/5Dr68kRzL6pfKh\nqvpEx5CpbGM96prKNjZbXdPaxvqsr2bi21h77+8AN/CThyH/fr0kWQgcBzzMEOvrYJ5gmS8PYAX7\nPun6evY8gXhz618MfIPRycPjW3vxBOs6mdExw5/bq//ZwHPH2l8Ezp5gXT/Fj78vczrwt23dLWR0\nkvUUfnwC8WWTqqvNP47ReYpnT2p9tZ/9SuC9+xkz8W2sZ10T38Z61jXxbaxPXdPYxoAlwKLWPgb4\na+AX9xpzEXuepL6qtV/Gniep7+UAT1IfcoeYknyE0aciTkiyHXgPoxM9VNWfMPqm9rmM/qM8CVzY\n5j2S5D8yuiYUwO/WnruUQ9f1bkbHEf94dL6J3TW6ENdSRruZMPoP8+Gq+swE63oT8K+S7Aa+D1xQ\no61x0Euj9KgL4JeAz1bV98ZeOuj6Al4F/BpwWztODPAuRr98p7mN9alrGttYn7qmsY31qQsmv42d\nCGzM6AZqRzD65f+pJL8LbKmqTcAVwAeTbGMUXhe0mu9IchVwJ7AbuKhGh6ueNr9JLUnqdDieg5Ak\n9WBASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqdP/Bz13j1C5l7pUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb90cac7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000 # top 30\n",
    "MAX_NB_WORDS = 20000 # more than vocab size\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('../result_all_windows_labels.csv')\n",
    "\n",
    "# Cleanup - remove no labels\n",
    "data_train = data_train[data_train['label'].notnull()]\n",
    "data_train = data_train[data_train.label != 'environmental']\n",
    "data_train = data_train[data_train.label != 'religious']\n",
    "data_train = data_train[data_train.label != 'economical']\n",
    "\n",
    "\n",
    "label_cat = {'violence/terrorism' : 1, 'misc': 2, 'political': 3, \n",
    "#              'religious': 4, 'economical': 5, 'environmental': 6\n",
    "            }\n",
    "print(label_cat) \n",
    "\n",
    "\n",
    "def to_category(x):\n",
    "    return label_cat[x]\n",
    "\n",
    "data_train['target'] = data_train.apply(lambda row: to_category(row['label']), axis=1)\n",
    "\n",
    "\n",
    "data_train['target'].plot.hist(alpha=0.5)\n",
    "\n",
    "\n",
    "texts = []\n",
    "# Get corpus by joining all keywords\n",
    "for index, row in data_train.iloc[ :, 2:32].iterrows():\n",
    "    texts.append(u' '.join(row.tolist()))\n",
    "    \n",
    "data_train['topicFlat'] = texts\n",
    "\n",
    "# texts = datdata_traina_train['topicFlat']\n",
    "\n",
    "labels = data_train['target']\n",
    "\n",
    "# print(labels)\n",
    "data_train['topicFlat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 17, 1, 2, 16, 42, 7, 47, 178, 56, 30, 1014, 395, 8, 160, 22, 38, 12, 201, 3300, 142, 10, 3, 17, 1640, 202, 433, 396, 307, 82, 145, 649]\n",
      "[1722, 4665, 4666, 157, 4667, 1792, 4668, 1596, 790, 4669, 2969, 2236, 4670, 137, 1173, 4671, 501, 719, 987, 2909, 4672, 4673, 394, 2911, 2912, 101, 2913, 790, 647, 1311, 1294, 2264, 2171, 1310, 2970, 790, 951, 790, 950, 2971, 4674, 2914, 1745, 43, 4675]\n",
      "Found 6077 unique tokens.\n",
      "Shape of data tensor: (1449, 1000)\n",
      "Shape of label tensor: (1449, 4)\n",
      "CPU times: user 104 ms, sys: 4 ms, total: 108 ms\n",
      "Wall time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# sequences = tokenizer.texts_to_matrix(texts, mode='tfidf')\n",
    "\n",
    "print(sequences[0])\n",
    "print (max(sequences, key=len))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index)) # all the tokens in corpus\n",
    "# print(word_index)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# print(data[0])\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[  0. 466. 290. 404.]\n",
      "[  0. 109.  67. 113.]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_test = data[-nb_validation_samples:]\n",
    "y_test = labels[-nb_validation_samples:]\n",
    "\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "CPU times: user 10.7 s, sys: 204 ms, total: 10.9 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation/data\n",
    "\n",
    "GLOVE_DIR = \"../data/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 15.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "\n",
    "# Embedding(5000, 32, input_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(label_cat) + 1, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model.summary()\n",
    "\n",
    "model_name = 'topicConvNet-Reg.h5'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "best_model = ModelCheckpoint(model_name, verbose=1, save_best_only=True)\n",
    "\n",
    "network_hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "          verbose=True, callbacks=[early_stop, best_model], validation_split=0.2,\n",
    "          epochs=20, batch_size=50)\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=50)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# print(network_hist.history)\n",
    "plot_history(network_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "# model = load_model('topicConvNet-Reg.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "# applying a more complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(filters=128,filter_length=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(filters=128, kernel_size=5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(filters=128, kernel_size=5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(label_cat) + 1, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - more complex convolutional neural network\")\n",
    "model.summary()\n",
    "\n",
    "model_name = 'topicConv_20.h5'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "best_model = ModelCheckpoint(model_name, verbose=1, save_best_only=True)\n",
    "\n",
    "network_hist = model.fit(x_train, y_train, validation_data=(x_train, y_train), \n",
    "          verbose=True, callbacks=[early_stop, best_model],\n",
    "          epochs=20, batch_size=50)\n",
    "\n",
    "score, acc = model.evaluate(x_train, y_train, batch_size=50)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# print(network_hist.history)\n",
    "plot_history(network_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EMBEDDING_DIM = 32\n",
    "# embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 32, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(len(label_cat) + 1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model_name = 'topicConv_simple.h5'\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "best_model = ModelCheckpoint(model_name, verbose=1, save_best_only=True)\n",
    "\n",
    "network_hist = model.fit(x_train, y_train, validation_data=(x_train, y_train), \n",
    "          verbose=True, callbacks=[early_stop, best_model],\n",
    "          epochs=20, batch_size=50)\n",
    "\n",
    "score, acc = model.evaluate(x_train, y_train, batch_size=50)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# print(network_hist.history)\n",
    "plot_history(network_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
