{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sonic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sonic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "download('punkt')\n",
    "download('stopwords') #download \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()]\n",
    "    return doc\n",
    "\n",
    "# Fetch ng20 dataset\n",
    "# ng20 = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'qoutes'))\n",
    "ng20 = fetch_20newsgroups() #nothing removed\n",
    "\n",
    "texts, y = ng20.data, ng20.target\n",
    "\n",
    "corpus = [preprocess(text) for text in texts]\n",
    "\n",
    "\n",
    "# print (corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading word2vec\n"
     ]
    }
   ],
   "source": [
    "# Centroid of the word vectors (Cosine Similarity)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word2vec_model.init_sims(replace=True) \n",
    "print(\"finish loading word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 total docs\n",
      "0 docs removed\n"
     ]
    }
   ],
   "source": [
    "# Doc filter vocab\n",
    "def filter_docs(corpus, texts, labels, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus, texts and labels given the function condition_on_doc which takes\n",
    "    a doc.\n",
    "    The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "    \n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "\n",
    "    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "    corpus = [doc for doc in corpus if len([word for word in doc if word in word2vec_model.vocab]) != 0]\n",
    "#     corpus = [doc for doc in corpus if len(doc) != 0]\n",
    "\n",
    "    print(\"{} total docs\".format(number_of_docs))\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts, labels)\n",
    "\n",
    "corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: (len(doc) != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10060 total docs\n",
      "4 docs removed\n"
     ]
    }
   ],
   "source": [
    "snippets = []\n",
    "snippets_labels = []\n",
    "snippets_file = \"data/data-web-snippets/train.txt\"\n",
    "with open(snippets_file, 'r') as f:\n",
    "    for line in f:\n",
    "        # each line is a snippet: a bag of words separated by spaces and\n",
    "        # the category\n",
    "        line = line.split()\n",
    "        category = line[-1]\n",
    "        doc = line[:-1]\n",
    "        snippets.append(doc)\n",
    "        snippets_labels.append(category)\n",
    "\n",
    "snippets, _, snippets_labels = filter_docs(snippets, None, snippets_labels, lambda doc: (len(doc) != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 total docs\n",
      "0 docs removed\n",
      "10056 total docs\n",
      "0 docs removed\n"
     ]
    }
   ],
   "source": [
    "# Clean data with respect to Word2Vec model vocab.\n",
    "corpus, texts, y = filter_docs(corpus, texts, y, lambda doc:has_vector_representation(word2vec_model,doc))\n",
    "snippets, _, snippets_labels = filter_docs(snippets,None, snippets_labels, lambda docs:has_vector_representation(word2vec_model, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSI\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "sims={'ng20':{}, 'snippets':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish calculating LSI\n"
     ]
    }
   ],
   "source": [
    "# NG20 - LSI\n",
    "dictionary=corpora.Dictionary(corpus)\n",
    "corpus_gensim=[dictionary.doc2bow(doc) for doc in corpus]\n",
    "tfidf=TfidfModel(corpus_gensim)\n",
    "corpus_tfidf=tfidf[corpus_gensim]\n",
    "lsi=LsiModel(corpus_tfidf, id2word=dictionary,num_topics=200)\n",
    "lsi_index=MatrixSimilarity(lsi[corpus_tfidf])\n",
    "\n",
    "sims['ng20']['LSI'] = np.array([lsi_index[lsi[corpus_tfidf[i]]] for i in range(len(corpus))])\n",
    "print ('finish calculating LSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish calculating LSI\n"
     ]
    }
   ],
   "source": [
    "# Snippets - LSI\n",
    "dictionary_snippets = corpora.Dictionary(snippets)\n",
    "corpus_gensim_snippets = [dictionary_snippets.doc2bow(doc) for doc in snippets]\n",
    "tfidf_snippets = TfidfModel(corpus_gensim_snippets)\n",
    "corpus_tfidf_snippets=tfidf_snippets[corpus_gensim_snippets]\n",
    "lsi_snippets = LsiModel(corpus_tfidf_snippets, id2word=dictionary_snippets, num_topics=200)\n",
    "lsi_index_snippets = MatrixSimilarity(lsi_snippets[corpus_tfidf_snippets])\n",
    "\n",
    "sims['snippets']['LSI'] = np.array([lsi_index[lsi[corpus_tfidf[i]]] for i in range(len(snippets))])\n",
    "print ('finish calculating LSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid of the word vectors (Cosine Similarity)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ng20 centroid matrix\n",
    "sims['ng20']['centroid']=cosine_similarity(np.array([document_vector(word2vec_model,doc) for doc in corpus]))\n",
    "\n",
    "\n",
    "sims['snippets']['centroid'] = cosine_similarity(np.array([document_vector(word2vec_model, doc) for doc in snippets]))\n",
    "\n",
    "print ('finish calculating cosin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0   958  7993  8266  8013   596  1082  1224  5553   659  8372  4627\n",
      "  7861  8555 10024  8364  6330  4985  3819  7878]\n",
      "[    0   958  7993  8266  8013   596  1082  1224  5553   659  8372  4627\n",
      "  7861  8555 10024  8364  6330  4985  3819  7878]\n",
      "[    0   958  2554  3112  7861 11225  3819  6418  5167  3424   730 11169\n",
      "  8153  1126  3311  4600  9580   659  8405  9456]\n",
      "[   0   13   15  973  378   17    2 6658 6829 6833 6307   16  974    8 5535\n",
      "   19   14    5  965   12]\n"
     ]
    }
   ],
   "source": [
    "def most_similar(i, X_sims, topn=None):\n",
    "    \"\"\"return the indices of the topn most similar documents with document i\n",
    "    given the similarity matrix X_sims\"\"\"\n",
    "\n",
    "    r = np.argsort(X_sims[i])[::-1]\n",
    "    if r is None:\n",
    "        return r\n",
    "    else:\n",
    "        return r[:topn]\n",
    "\n",
    "#LSI\n",
    "print(most_similar(0, sims['ng20']['LSI'], 20))\n",
    "print(most_similar(0, sims['snippets']['LSI'], 20))\n",
    "\n",
    "#Centroid\n",
    "print(most_similar(0, sims['ng20']['centroid'], 20))\n",
    "print(most_similar(0, sims['snippets']['centroid'], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WMD\n",
    "from gensim.similarities import WmdSimilarity\n",
    "\n",
    "wmd_similarity_top20 = WmdSimilarity(corpus, word2vec_model, num_best=20)\n",
    "most_similars_wmd_ng20_top20 = wmd_similarity_top20[corpus[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_similarity_snippets = WmdSimilarity(snippets, word2vec_model, num_best=20)\n",
    "most_similars_snippets = wmd_similarity_snippets[snippets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.0),\n",
       " (13, 0.60329492929644768),\n",
       " (973, 0.52862723340180551),\n",
       " (2, 0.52601643645020979),\n",
       " (378, 0.52323760224834504),\n",
       " (16, 0.5177545982856413),\n",
       " (7509, 0.51254049902118537),\n",
       " (12, 0.51098133962722492),\n",
       " (6828, 0.50705399681485708),\n",
       " (19, 0.50653771271015224),\n",
       " (17, 0.5065278697576826),\n",
       " (974, 0.50597894422453926),\n",
       " (7, 0.5041686532181463),\n",
       " (15, 0.5041521835206122),\n",
       " (712, 0.50320841421252649),\n",
       " (6663, 0.50302378894798072),\n",
       " (56, 0.50239767268269619),\n",
       " (6829, 0.50216514047488248),\n",
       " (5, 0.501937836425843),\n",
       " (2169, 0.50181756077408612)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similars_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
